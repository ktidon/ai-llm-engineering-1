{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### AI/LLM Engineering Kick-off!! \n",
        "\n",
        "\n",
        "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, you'll need an OpenAI API Key. [here](https://platform.openai.com)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecnJouXnUgKv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ API key set successfully!\n",
            "API key in environment: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "# Set API key directly in environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = \n",
        "\n",
        "# For the new OpenAI client, the API key is automatically read from environment\n",
        "# No need to set openai.api_key for the new client\n",
        "\n",
        "print(\"✅ API key set successfully!\")\n",
        "print(f\"API key in environment: {'OPENAI_API_KEY' in os.environ}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BzNh0ADtEKYkI74ZPvmxmjCLiBMP9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='As of my knowledge cutoff in October 2023, here\\'s a comparison between LangChain and LLangGraph:\\n\\n**LangChain:**\\n- **Purpose:** A comprehensive framework designed to develop applications that leverage large language models (LLMs). It provides tools for prompt management, memory, chains (sequences of operations), agents, and integrations with various data sources.\\n- **Features:**\\n  - Modular components for building LLM-based applications.\\n  - Support for memory, retrieval, and document processing.\\n  - Easy integration with APIs, databases, and other tools.\\n  - Facilitates building chatbots, question-answering systems, and more.\\n- **Use Cases:** Chatbots, conversational agents, document understanding, code analysis, etc.\\n\\n**LLangGraph (or similar tools with a focus on graph-based LLM workflows):**\\n- **Purpose:** A more specialized or conceptual toolset (depending on exact context) focused on representing and managing workflows or data transformations involving LLMs as directed graphs or workflows.\\n- **Features:**\\n  - Visual or programmatic graph-based modeling of processes involving LLMs.\\n  - Emphasizes dependency tracking, modularity, and visualization of complex pipelines.\\n  - Might be used to orchestrate or optimize collections of LLM calls.\\n- **Use Cases:** Complex data processing pipelines, structured workflows, visualizing dependencies between steps in LLM applications.\\n\\n---\\n\\n### Key Differences:\\n- **Scope & Generality:**  \\n  LangChain is a broad, well-established framework aimed at building diverse LLM applications. LLangGraph (or similar graph-based tools) tend to focus more on representing workflows or pipelines visually or structurally, possibly as an extension or component within an LLM application.\\n\\n- **Functionality:**  \\n  LangChain provides the building blocks for interacting with LLMs and managing application state, memory, and data flow. LLangGraph emphasizes workflow modeling, visualization, and dependency management, potentially atop or alongside frameworks like LangChain.\\n\\n- **Usage Context:**  \\n  Use LangChain when developing applications that require flexible LLM integrations, data retrieval, or conversational capabilities. Use LLangGraph when designing, visualizing, or managing complex LLM workflows or pipelines, especially when clarity of dependencies and modularity are priorities.\\n\\n---\\n\\n### Note:\\nThe specifics of \"LLangGraph\" might vary or be a proprietary or niche tool; if you have a link or more context, I can provide more precise insights.\\n\\n**Summary:**  \\n- **LangChain:** General-purpose LLM application development framework.  \\n- **LLangGraph:** Likely a tool or methodology for modeling LLM workflows as graphs/pipelines.\\n\\nLet me know if you\\'d like more details or clarification!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753968782, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=545, prompt_tokens=19, total_tokens=564, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LLangGraph?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "As of my knowledge cutoff in October 2023, **LangChain** and **LLaMaGraph** are two distinct tools or frameworks related to language models and their applications, but they serve different purposes:\n",
              "\n",
              "### LangChain\n",
              "- **Purpose:** A comprehensive framework for building applications with large language models (LLMs). It facilitates chaining together prompts, managing memory, interacting with external data sources, and deploying conversational agents.\n",
              "- **Features:**\n",
              "  - Modular components for prompt management\n",
              "  - Tools for integrating with APIs, databases, and other data sources\n",
              "  - Support for memory management to enable context-aware conversations\n",
              "  - Facilitates the development of chatbots, question-answering systems, and more\n",
              "- **Use Cases:** Building complex LLM-powered applications, chatbots, agents, and workflows that involve multiple steps and data sources.\n",
              "\n",
              "### LLaMaGraph\n",
              "- **Purpose:** A tool designed for working with graph-based data structures and integrating large language models with graph reasoning or generation tasks.\n",
              "- **Features:**\n",
              "  - Enables LLMs to understand, generate, or manipulate graph data\n",
              "  - Suitable for tasks like knowledge graph construction, reasoning over graph structures, or visualizing relationships\n",
              "  - Often used in contexts where structured data representations (like graphs) are central\n",
              "- **Use Cases:** Knowledge graph augmentation, graph-based question answering, reasoning over interconnected data\n",
              "\n",
              "---\n",
              "\n",
              "### Key Differences\n",
              "| Aspect | LangChain | LLaMaGraph |\n",
              "|---------|--------------|------------|\n",
              "| Primary Focus | Building LLM-powered applications and workflows | Working with graph-structured data and reasoning |\n",
              "| Core Functionality | Prompt orchestration, memory, API integration | Graph data manipulation, reasoning, generation |\n",
              "| Use Cases | Chatbots, multi-step workflows, data integration | Knowledge graphs, graph reasoning, structured data tasks |\n",
              "\n",
              "---\n",
              "\n",
              "### Summary\n",
              "**LangChain** is a versatile framework for developing applications around large language models, emphasizing prompt management and API integration. **LLaMaGraph** (or similar graph-focused tools) is specialized for tasks involving graphs and structured data in conjunction with language models.\n",
              "\n",
              "**Note:** If you were referring to \"LLaMaGraph,\" and it's a newer or different project, I recommend checking the latest official documentation for the most accurate and detailed information."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LLangGraph?\"\n",
        "\n",
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me? I can't believe I even have to choose between crushed ice and cubed ice when I'm this starving! All I want is something to eat, not some pointless ice debate! Just give me something to satisfy my hunger already!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice is really fun and refreshing because it feels so cool and easy to enjoy! But cubed ice is great too—it's perfect for keeping drinks cold without diluting them too quickly. Both have their own charm—depends on the mood! What's your favorite?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-BzNhnWxuiBGvaUKb1D1Hpy01hVmC4', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I think crushed ice is really fun and refreshing because it feels so cool and easy to enjoy! But cubed ice is great too—it's perfect for keeping drinks cold without diluting them too quickly. Both have their own charm—depends on the mood! What's your favorite?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753968831, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=55, prompt_tokens=30, total_tokens=85, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Climate change refers to significant and lasting changes in global weather patterns and temperatures, primarily driven by human activities such as burning fossil fuels, deforestation, and industrial processes. These activities increase the concentration of greenhouse gases like carbon dioxide in the atmosphere, leading to global warming. The effects of climate change include more frequent extreme weather events, rising sea levels, melting glaciers, and disrupted ecosystems. Addressing climate change requires international cooperation, sustainable practices, and efforts to reduce greenhouse gas emissions to protect the planet for future generations."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Ay, naku! Mga kababayan, alam nyo ba? Climate change na talaga ang usapin nating lahat ngayon! Parang bongga pero, seryoso, basta-basta lang—kailangan nating magbago. Hindi pwedeng tamad-tamad lang tayo sa kalikasan, ha! Kaya magtulungan tayo, mag-planting, iwasan ang plastic, at maging mindful sa mga energy na ginagamit. Kasi kung hindi, baka mag-cross over na ang ating world sa isang napakalaking problema – parang isang nakakaiyak na teleserye! So, halina’t sama-sama tayo, tumutok sa solusyon, at i-love ang Earth para sa mas brighter at healthier na bukas!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench was perfect for gripping the falbean to secure the rotating joint smoothly."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are 2 letter \"r\"s in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "Notice that the model cannot count properly. It counted only 2 r's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #2: Update the prompt so that it can count correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's count the number of r's in \"strawberry.\"\n",
              "\n",
              "strawberry = s + t + r + a + w + b + e + r + r + y\n",
              "\n",
              "Counting the r's:\n",
              "\n",
              "- 1st r at position 3\n",
              "- 2nd r at position 8\n",
              "- 3rd r at position 9\n",
              "\n",
              "Total r's: 3\n",
              "\n",
              "**Therefore, there are 3 r's in \"strawberry.\"**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's count the number of r's in \"strawberry.\"\n",
              "\n",
              "strawberry = s + t + r + a + w + b + e + r + r + y\n",
              "\n",
              "Letters: s, t, r, a, w, b, e, r, r, y\n",
              "\n",
              "Number of r's: 3\n",
              "\n",
              "**Therefore, there are 3 r's in \"strawberry.\"**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "\n",
        "You will count the number of letters.\n",
        "\n",
        "Example:\n",
        "how many c's in clock?\n",
        "clock = c + l + o + c + k\n",
        "therefore,  2 c's\n",
        "\n",
        "how many a's in ants?\n",
        "ants = a + n + t + s\n",
        "therefore, 1 a\n",
        "\n",
        "how many e's in elephant?\n",
        "elephant = e + l + e + p + h + a + n + t\n",
        "therefore, 2 e's\n",
        "\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
